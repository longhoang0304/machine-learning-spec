**1. Here is some code that you saw in the lecture:**
![](./Imgs/C2W2Q2_Img1.png)

**Which of the following activation functions is the most common choice for the hidden layers of a neural network?**

- [ ] Sigmoid
- [ ] Most hidden layers do not use any activation function
- [ ] Linear
- [x] ReLU (rectified linear unit)

Note: Yes! A ReLU is most often used because it is faster to train compared to the sigmoid. This is because the ReLU is only flat on one side (the left side) whereas the sigmoid goes flat (horizontal, slope approaching zero) on both sides of the curve.

**2. Here is code that you saw in the lecture:**

![](./Imgs/C2W2Q2_Img2.png)

**For the task of predicting housing prices, which activation functions could you choose for the output layer? Choose the 2 options that apply.**

- [x] linear 
  - Note: Yes! A linear activation function can be used for a regression task where the output can be both negative and positive, but it's also possible to use it for a task where the output is 0 or greater (like with house prices).
- [x] ReLU
  - Note: Yes! ReLU outputs values 0 or greater, and housing prices are positive values.
- [ ] Sigmoid

**3. True/False? A neural network with many layers but no activation function (in the hidden layers) is not effective; thatâ€™s why we should instead use the linear activation function in every hidden layer.**
- [x] False
- [ ] True

Note: Yes! A neural network with many layers but no activation function is not effective. A linear activation is the same as "no activation function".
